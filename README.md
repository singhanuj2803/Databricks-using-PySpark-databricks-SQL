# Databricks-using-PySpark-SQL

Welcome to my project on Databricks & Apache spark -Build ETL data pipeline

1.	Databricks combines the best of data warehouses and data lakes into a Lakehouse architecture. In this project I will be demonstrating how to perform various operations in Scala, Python and Spark SQL. 
2.	This will help every developer in building solutions which will create value and mindset to build Batch Process in any of the language. 
3.	This project will help in writing same commands in different language and based on your client needs we can adopt and deliver world class solution. 
4.	We will be building end to end solution in azure databricks.

## Key Points

- We will be building our own cluster which will process our data and with one click operation we will load different sources data to Azure SQL and Delta tables
- After that we will be leveraging databricks notebook to prepare dashboard to answer business questions
- Based on the needs we will be deploying infrastructure on Azure cloud
- These scenarios will give developers 360 degree exposure on cloud platform and how to step up various resources
- All activities are performed in Azure Databricks

## Fundamentals
-	Databricks
-	Delta tables
-	Concept of versions and vacuum on delta tables
-	Apache Spark SQL
-	Filtering Dataframe
-	Renaming, drop, Select, Cast
-	Aggregation operations SUM, AVERAGE, MAX, MIN
-	Rank, Row Number, Dense Rank
-	Building dashboards
-	Analytics
This project is suitable for Data engineers, BI architect, Data Analyst, ETL developer, BI Manager

## Solution_Architecture_For_Project

<img width="856" alt="Solution_Architecture_For_Project" src="https://github.com/singhanuj2803/Databricks-using-PySpark-SQL/assets/86510801/08dbbc89-e527-4de1-9250-7342aca9d20e">

## Project_Description


<img width="847" alt="Project_Description" src="https://github.com/singhanuj2803/Databricks-using-PySpark-SQL/assets/86510801/a240c54a-d2d1-4697-98d8-ce7b493b1c72">
